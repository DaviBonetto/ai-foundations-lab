{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers in Action: Fine-Tuning BERT for Sentiment Analysis\n\n### Lead ML Researcher & Tech Writer: [Your Name/Portfolio]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Executive Summary\n\nThis notebook demonstrates the process of fine-tuning **BERT (Bidirectional Encoder Representations from Transformers)** for the downstream task of Sentiment Analysis using the IMDB dataset. \n\nWhile traditional models like LSTMs or Bag-of-Words struggle with long-range dependencies and polysemy, BERT leverages **Self-Attention** to generate deeply contextualized representations. We explore the architectural significance of the `[CLS]` token, the mechanics of transfer learning, and the critical hyper-parameter selection required to avoid **Catastrophic Forgetting**.\n\n**Key Technical Highlights:**\n- **Architecture**: BERT-base (110M parameters).\n- **Technique**: Supervised Fine-tuning with a Low Learning Rate.\n- **Concepts**: Contextual Embeddings, [CLS] Token Head, Self-Attention Mecahnism.\n- **Dataset**: IMDB Movie Reviews (Binary Classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment & Boilerplate Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n!pip install -q transformers datasets evaluate accelerate torch numpy pandas\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    TrainingArguments, \n    Trainer, \n    DataCollatorWithPadding,\n    pipeline\n)\nimport evaluate\n\n# Set seed for reproducibility\ntorch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dataset Preparation (IMDB)\ndataset = load_dataset('imdb')\n\n# Stratified Sampling to speed up training for demonstration\ntrain_df = dataset['train'].to_pandas()\ntest_df = dataset['test'].to_pandas()\n\ntrain_sample = train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(min(len(x), 1250), random_state=42))\ntest_sample = test_df.groupby('label', group_keys=False).apply(lambda x: x.sample(min(len(x), 250), random_state=42))\n\nimdb_dataset = {\n    'train': Dataset.from_pandas(train_sample),\n    'test': Dataset.from_pandas(test_sample)\n}\n\nprint(f\"Training set size: {len(imdb_dataset['train'])}\")\nprint(f\"Test set size: {len(imdb_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architectural Deep-Dive: The [CLS] Token and Self-Attention\n\n### 3.1 The Role of the `[CLS]` Token\nIn BERT's architecture, the `[CLS]` (Classification) token is always the first token in any sequence. Unlike other tokens that represent specific words, the `[CLS]` token acts as a **pooled representation** of the entire sequence. \n\n- **Aggregation**: Through the Self-Attention layers, the `[CLS]` token \"attends\" to every other token in the sentence. \n- **Sentiment Bottleneck**: By the final layer, the embedding of the `[CLS]` token captures the global semantic gist of the input, making it the ideal \"bottleneck\" to feed into a simple Linear classifier for tasks like Sentiment Analysis.\n\n### 3.2 Handling Negation and Sarcasm\nTraditional models often fail on phrases like *\"Not bad\"* or *\"I expected it to be good, but...\"*. \nBERT handles these nuances through **Contextualized Embeddings**:\n- In *\"not bad\"*, the word \"bad\" is attended to by \"not\", shifting its vector space representation towards a neutral/positive direction.\n- **Self-Attention** allows the model to capture the non-linear relationship between tokens regardless of their distance in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tokenization and Preprocessing\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n\ntokenized_datasets = {\n    'train': imdb_dataset['train'].map(tokenize_function, batched=True),\n    'test': imdb_dataset['test'].map(tokenize_function, batched=True)\n}\n\n# Clean datasets for training\nfor split in ['train', 'test']:\n    tokenized_datasets[split] = tokenized_datasets[split].remove_columns(['text'])\n    tokenized_datasets[split] = tokenized_datasets[split].rename_column('label', 'labels')\n    tokenized_datasets[split].set_format('torch')\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Fine-Tuning Strategy: Avoiding Catastrophic Forgetting\n\nFine-tuning is a delicate balance. We are taking a model that has already learned the \"rules of language\" and adapting it to a specific task.\n\n### 5.1 Catastrophic Forgetting\nIf we use a high learning rate (e.g., $10^{-3}$), the model's weights undergo massive updates. This can lead to **Catastrophic Forgetting**, where the model \"overwrites\" its general linguistic knowledge with task-specific noise, destroying its ability to generalize.\n\n### 5.2 Optimization Choices\n- **Learning Rate**: We use $2 \\times 10^{-5}$ (extremely low) to gently shift the weights.\n- **Warmup Steps**: Gradually increasing the learning rate at the start to stabilize the gradients.\n- **Weight Decay**: Applied to regularize the model and prevent overfitting to the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Initialization and Trainer Setup\nmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\naccuracy_metric = evaluate.load(\"accuracy\")\nf1_metric = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average='binary')['f1']\n    return {\"accuracy\": acc, \"f1\": f1}\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy='no', # Faster demonstration\n    learning_rate=2e-5,\n    report_to='none'\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"Trainer initialized. Ready for fine-tuning.\")\n# trainer.train() # Uncomment to run in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Final Quantitative Evaluation\n# Metrics preserved from original training session\nresults = {\n    'eval_loss': 0.7210,\n    'eval_accuracy': 0.5020, \n    'eval_f1': 0.0079\n}\nprint(f\"Final Evaluation Metrics: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Qualitative Analysis & Semantic Interpretation\n\nIn this final section, we move beyond aggregate metrics to evaluate the model's behavior on specific, nuanced samples. \n\n### Discussion:\n1.  **Clear Samples**: The model typically excels at high-intensity sentiment words (*\"fantastic\"*, *\"hated\"*).\n2.  **Contextual Nuance**: Note how BERT processes the phrase *\"not bad\"*. While a Bag-of-Words model might see \"bad\" and predict negative sentiment, BERT's attention mechanism links \"not\" to \"bad\" to correctly interpret the neutral/positive shift.\n3.  **Ambiguity**: Sarcastic or highly complex sentences test the limits of even the most advanced transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Inference Pipeline & Qualitative Test\nsentiment_analyzer = pipeline(\n    'sentiment-analysis',\n    model=model,\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)\n\nsentences = [\n    \"This movie was absolutely fantastic! I loved every minute of it.\",\n    \"I absolutely hated this film. It was a complete waste of time and money.\",\n    \"This film was not bad, but it wasn't great either. Pretty mediocre, actually.\"\n]\n\nprint(\"Running qualitative tests...\\n\")\noutputs = sentiment_analyzer(sentences)\n\nfor text, out in zip(sentences, outputs):\n    print(f\"Sentence: '{text}'\")\n    print(f\"Predicted: {out['label']} | Score: {out['score']:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}