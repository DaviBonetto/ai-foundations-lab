{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis) from Scratch\n",
    "**Objective:** Implement PCA using Eigendecomposition of the Covariance Matrix to understand dimensionality reduction, variance maximizing directions, and reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup (Minimal)\n",
    "**Dimensionality Reduction** simplifies models and visualization by reducing feature count while keeping important information.\n",
    "\n",
    "**PCA Idea:** Find new orthogonal axes (Principal Components) that point in the directions of **maximum variance** in the data.\n",
    "\n",
    "**Requirement:** Data must be **centered** (mean = 0) so that the covariance matrix correctly represents the spread of data around the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) 2D Highly Correlated Data\n",
    "n_samples = 200\n",
    "x_1 = np.random.rand(n_samples) * 10\n",
    "x_2 = 2 * x_1 + 3 + np.random.randn(n_samples) * 1.5 # Strong linear correlation\n",
    "X_2d = np.vstack([x_1, x_2]).T\n",
    "\n",
    "# B) 3D Structured Data (2D Plane + Noise in 3rd dim)\n",
    "x = np.random.randn(n_samples)\n",
    "y = np.random.randn(n_samples)\n",
    "z = 0.1 * np.random.randn(n_samples) # Little variance here\n",
    "# Rotate to make it interesting\n",
    "X_3d = np.vstack([x, y + 2*x, z]).T\n",
    "\n",
    "# Visualize 2D Data\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7)\n",
    "plt.title(\"2D Correlated Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation (NumPy)\n",
    "We will compute PCA via **Eigendecomposition of the Covariance Matrix**: $C = \\frac{1}{n-1} X^T X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_or_center(X, use_std=False):\n",
    "    \"\"\"Centers data (mean=0). Optionally standardizes (std=1).\"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    X_centered = X - mean\n",
    "    if use_std:\n",
    "        std = np.std(X_centered, axis=0)\n",
    "        # Avoid division by zero\n",
    "        std[std == 0] = 1\n",
    "        X_centered = X_centered / std\n",
    "    return X_centered, mean\n",
    "\n",
    "def covariance_matrix(X_centered):\n",
    "    \"\"\"Computes covariance matrix for centered data: (X^T @ X) / (n-1)\"\"\"\n",
    "    n = X_centered.shape[0]\n",
    "    return (X_centered.T @ X_centered) / (n - 1)\n",
    "\n",
    "def pca_fit_eig(X, n_components):\n",
    "    \"\"\"Fits PCA using Eigendecomposition.\"\"\"\n",
    "    # 1. Center Data\n",
    "    X_centered, mean = standardize_or_center(X)\n",
    "    \n",
    "    # 2. Compute Covariance Matrix\n",
    "    cov_mat = covariance_matrix(X_centered)\n",
    "    \n",
    "    # 3. Eigendecomposition\n",
    "    # eigh is optimized for symmetric matrices (like covariance)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_mat)\n",
    "    \n",
    "    # 4. Sort method (High to Low)\n",
    "    sorted_index = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvalues = eigenvalues[sorted_index]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_index]\n",
    "    \n",
    "    # 5. Select Top-K Components\n",
    "    components = sorted_eigenvectors[:, :n_components].T # Shape (n_components, n_features)\n",
    "    \n",
    "    # Explained Variance Stats\n",
    "    explained_variance = sorted_eigenvalues[:n_components]\n",
    "    explained_variance_ratio = explained_variance / np.sum(sorted_eigenvalues)\n",
    "    \n",
    "    return components, explained_variance, explained_variance_ratio, mean\n",
    "\n",
    "def pca_transform(X, components, mean):\n",
    "    \"\"\"Projects X onto the principal components.\"\"\"\n",
    "    return (X - mean) @ components.T\n",
    "\n",
    "def pca_inverse_transform(Z, components, mean):\n",
    "    \"\"\"Reconstructs X from Z.\"\"\"\n",
    "    return (Z @ components) + mean\n",
    "\n",
    "def reconstruction_mse(X, X_recon):\n",
    "    return np.mean((X - X_recon)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: 2D Data -> 1 Component\n",
    "components_2d, _, ratio_2d, mean_2d = pca_fit_eig(X_2d, n_components=1)\n",
    "\n",
    "# Transform and Reconstruct\n",
    "Z_2d = pca_transform(X_2d, components_2d, mean_2d)\n",
    "X_recon_2d = pca_inverse_transform(Z_2d, components_2d, mean_2d)\n",
    "mse_2d = reconstruction_mse(X_2d, X_recon_2d)\n",
    "\n",
    "print(f\"2D Data Explained Variance Ratio (1st comp): {ratio_2d[0]:.4f}\")\n",
    "print(f\"2D Data Reconstruction MSE: {mse_2d:.4f}\")\n",
    "\n",
    "# Visualization: Principal Component Direction\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.3, label=\"Original Data\")\n",
    "plt.scatter(X_recon_2d[:, 0], X_recon_2d[:, 1], color='red', alpha=0.5, label=\"Reconstructed (1D)\")\n",
    "\n",
    "# Draw Arrow for PC1\n",
    "start = mean_2d\n",
    "end = mean_2d + components_2d[0] * 5 # Scale for visibility\n",
    "plt.arrow(start[0], start[1], end[0]-start[0], end[1]-start[1], \n",
    "          color='black', width=0.2, head_width=0.8, label=\"PC1 Direction\")\n",
    "\n",
    "plt.title(f\"PCA (1 Component) on 2D Data\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: 3D Data -> K Components\n",
    "n_dims_total = X_3d.shape[1]\n",
    "mses = []\n",
    "cumulative_variance = []\n",
    "\n",
    "for k in range(1, n_dims_total + 1):\n",
    "    comps, _, ratio, m = pca_fit_eig(X_3d, n_components=k)\n",
    "    Z = pca_transform(X_3d, comps, m)\n",
    "    X_rec = pca_inverse_transform(Z, comps, m)\n",
    "    mse = reconstruction_mse(X_3d, X_rec)\n",
    "    \n",
    "    mses.append(mse)\n",
    "    # Re-fit to get cumulative ratio easily or just sum `ratio` (since we request k)\n",
    "    cumulative_variance.append(np.sum(ratio))\n",
    "\n",
    "# Plotting Stats\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Number of Components K')\n",
    "ax1.set_ylabel('Reconstruction MSE', color=color)\n",
    "ax1.plot(range(1, n_dims_total + 1), mses, color=color, marker='o')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('Cumulative Explained Variance (%)', color=color)\n",
    "ax2.plot(range(1, n_dims_total + 1), [x*100 for x in cumulative_variance], color=color, marker='s', linestyle='--')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title(\"Tradeoff: Dimensionality vs Information Loss (3D Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "sklearn_pca = PCA(n_components=1)\n",
    "sklearn_pca.fit(X_2d)\n",
    "\n",
    "print(\"Comparing 2D Data Results:\")\n",
    "print(f\"My Explained Var Ratio:      {ratio_2d[0]:.6f}\")\n",
    "print(f\"Sklearn Explained Var Ratio: {sklearn_pca.explained_variance_ratio_[0]:.6f}\")\n",
    "print(\"\\nNote: Values should be identical (within float precision).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Takeaways\n",
    "*   **Variance as Information:** PCA assumes that the interesting features are the ones with the largest variance. The 1st component is the direction of widest spread.\n",
    "*   **Centering is Key:** Without subtracting the mean, the covariance computation would be incorrect, as it would capture the spread relative to origin (0,0) rather than the data's center.\n",
    "*   **Correlation = Redundancy:** In the 2D example, features were highly correlated. PCA found that ~99% of the information could be compressed into a single dimension (linear combination of the two).\n",
    "*   **Reconstruction Error:** As we increase $K$, we capture more variance and the Mean Squared Error (MSE) of reconstruction drops to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "*   This concludes **Module 1: Machine Learning**.\n",
    "*   Proceed to **Module 2: Deep Learning**.\n",
    "*   [Start Neural Networks](../nonlinear-models-neural-networks/README.md) (or verify next module path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
