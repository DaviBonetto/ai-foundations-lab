{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM & Kernels (Concepts + Practical Implementations)\n",
    "**Objective:** Implement Linear SVM (Pegasos) and Kernel Perceptron from scratch to understand margins, hinge loss, and the kernel trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts (Minimal)\n",
    "**Support Vector Machines (SVM)** aim to find the hyperplane that maximizes the **margin** between classes.\n",
    "\n",
    "**Hinge Loss:**\n",
    "Typical loss for SVM. It penalizes points that are on the wrong side of the margin.\n",
    "$$L(y, f(x)) = \\max(0, 1 - y \\cdot f(x))$$\n",
    "where $y \\in \\{-1, 1\\}$.\n",
    "\n",
    "**Kernel Trick:**\n",
    "Projects data into a higher-dimensional space where it becomes linearly separable, without computing the coordinates explicitly. It replaces the dot product $\\mathbf{x} \\cdot \\mathbf{x}'$ with a kernel function $K(\\mathbf{x}, \\mathbf{x}')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data (Linear vs Non-linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Datasets\n",
    "n_samples = 200\n",
    "\n",
    "# 1. Linear Data (Blobs)\n",
    "X_lin, y_lin = make_blobs(n_samples=n_samples, centers=2, random_state=6, cluster_std=1.2)\n",
    "y_lin = np.where(y_lin == 0, -1, 1) # Convert to {-1, 1}\n",
    "\n",
    "# 2. Non-linear Data (Moons)\n",
    "X_nl, y_nl = make_moons(n_samples=n_samples, noise=0.15, random_state=42)\n",
    "y_nl = np.where(y_nl == 0, -1, 1)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_lin[y_lin==-1][:, 0], X_lin[y_lin==-1][:, 1], color='red', label='-1')\n",
    "plt.scatter(X_lin[y_lin==1][:, 0], X_lin[y_lin==1][:, 1], color='blue', label='+1')\n",
    "plt.title(\"Linear Data (Blobs)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_nl[y_nl==-1][:, 0], X_nl[y_nl==-1][:, 1], color='red', label='-1')\n",
    "plt.scatter(X_nl[y_nl==1][:, 0], X_nl[y_nl==1][:, 1], color='blue', label='+1')\n",
    "plt.title(\"Non-linear Data (Moons)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1: Linear SVM via Pegasos (NumPy)\n",
    "We use the **Pegasos** algorithm (Primal Estimated sub-GrAdient SOlver for SVM), a stochastic gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(y, scores):\n",
    "    return np.mean(np.maximum(0, 1 - y * scores))\n",
    "\n",
    "def fit_pegasos(X, y, lambda_reg=0.01, epochs=1000, lr=0.01):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)\n",
    "    b = 0\n",
    "    history = []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        # Stochastic selection (one sample or mini-batch)\n",
    "        idx = np.random.randint(0, m)\n",
    "        x_i = X[idx]\n",
    "        y_i = y[idx]\n",
    "        \n",
    "        # Decision condition\n",
    "        # If strictly < 1, inside functional margin or wrong side -> update\n",
    "        condition = y_i * (np.dot(x_i, w) + b) < 1\n",
    "        \n",
    "        if condition:\n",
    "            w = (1 - lr * lambda_reg) * w + lr * y_i * x_i\n",
    "            b = b + lr * y_i\n",
    "        else:\n",
    "            w = (1 - lr * lambda_reg) * w\n",
    "        \n",
    "        # Record loss occasionally\n",
    "        scores = X.dot(w) + b\n",
    "        loss = hinge_loss(y, scores) + (lambda_reg / 2) * np.dot(w, w)\n",
    "        history.append(loss)\n",
    "        \n",
    "    return w, b, history\n",
    "\n",
    "def predict_linear(X, w, b):\n",
    "    scores = X.dot(w) + b\n",
    "    return np.sign(scores)\n",
    "\n",
    "# Train on Linear Data\n",
    "w_lin, b_lin, hist_lin = fit_pegasos(X_lin, y_lin, lambda_reg=0.01, epochs=2000, lr=0.01)\n",
    "\n",
    "y_pred_lin = predict_linear(X_lin, w_lin, b_lin)\n",
    "acc_lin = np.mean(y_pred_lin == y_lin)\n",
    "print(f\"Linear SVM Accuracy: {acc_lin:.2f}\")\n",
    "\n",
    "# Visualization of Decision Boundary\n",
    "def plot_boundary(X, y, w, b, title):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', alpha=0.7)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # Create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = (xy.dot(w) + b).reshape(XX.shape)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_boundary(X_lin, y_lin, w_lin, b_lin, f\"Pegasos Linear SVM (Acc: {acc_lin:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2: Kernelized Model (Kernel Perceptron)\n",
    "Since standard SVM quadratic programming is complex, we implement the **Kernel Perceptron**. It's a simpler algorithm that supports kernels and demonstrates the core concept: learning weights in the dual space ($\\alpha$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def rbf_kernel(x1, x2, gamma=1.0):\n",
    "    # Vectorized computation of ||x1 - x2||^2\n",
    "    diff = x1 - x2\n",
    "    return np.exp(-gamma * np.dot(diff, diff))\n",
    "\n",
    "def fit_kernel_perceptron(X, y, kernel_func=rbf_kernel, epochs=10, gamma=1.0):\n",
    "    m = X.shape[0]\n",
    "    alpha = np.zeros(m)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for i in range(m):\n",
    "            # Compute prediction based on current alphas\n",
    "            # Sum_{j} (alpha_j * y_j * K(x_j, x_i))\n",
    "            prediction_score = 0\n",
    "            # Optimization: could precompute Kernel Matrix for speed, but loop is clearer for \"from scratch\"\n",
    "            for j in range(m):\n",
    "                if alpha[j] != 0:\n",
    "                    k_val = kernel_func(X[j], X[i], gamma=gamma) if kernel_func == rbf_kernel else kernel_func(X[j], X[i])\n",
    "                    prediction_score += alpha[j] * y[j] * k_val\n",
    "            \n",
    "            # Perceptron update rule: if sign mismatch\n",
    "            if y[i] * prediction_score <= 0:\n",
    "                alpha[i] += 1\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "def predict_kernel(X_train, y_train, X_test, alpha, kernel_func=rbf_kernel, gamma=1.0):\n",
    "    y_pred = []\n",
    "    for x_t in X_test:\n",
    "        score = 0\n",
    "        for i in range(len(alpha)):\n",
    "            if alpha[i] != 0:\n",
    "                 k_val = kernel_func(X_train[i], x_t, gamma=gamma) if kernel_func == rbf_kernel else kernel_func(X_train[i], x_t)\n",
    "                 score += alpha[i] * y_train[i] * k_val\n",
    "        y_pred.append(np.sign(score))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "# Train on Non-Linear Data (Moons)\n",
    "gamma_val = 2.0\n",
    "alpha_rbf = fit_kernel_perceptron(X_nl, y_nl, kernel_func=rbf_kernel, epochs=5, gamma=gamma_val)\n",
    "\n",
    "# Predict on grid for visualization\n",
    "def plot_kernel_boundary(X, y, alpha, gamma, title):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "    \n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # Note: Predicting on entire grid is slow O(N_train * N_grid), but acceptable for small N\n",
    "    Z = predict_kernel(X, y, grid_points, alpha, kernel_func=rbf_kernel, gamma=gamma)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_kernel_boundary(X_nl, y_nl, alpha_rbf, gamma_val, f\"Kernel Perceptron (RBF) - Educational Implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Comparison (sklearn)\n",
    "Comparing our implementation against optimized solvers (SVC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linear Baseline\n",
    "svc_lin = SVC(kernel='linear', C=1.0)\n",
    "svc_lin.fit(X_lin, y_lin)\n",
    "acc_base_lin = svc_lin.score(X_lin, y_lin)\n",
    "\n",
    "# 2. RBF Baseline\n",
    "svc_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svc_rbf.fit(X_nl, y_nl)\n",
    "acc_base_rbf = svc_rbf.score(X_nl, y_nl)\n",
    "\n",
    "print(f\"Sklearn Linear Acc: {acc_base_lin:.2f} (vs Our Pegasos: {acc_lin:.2f})\")\n",
    "print(f\"Sklearn RBF Acc:    {acc_base_rbf:.2f}\")\n",
    "\n",
    "# Visualize Sklearn RBF Boundary\n",
    "plt.figure(figsize=(6, 5))\n",
    "xx, yy = np.meshgrid(np.linspace(X_nl[:,0].min()-1, X_nl[:,0].max()+1, 50),\n",
    "                     np.linspace(X_nl[:,1].min()-1, X_nl[:,1].max()+1, 50))\n",
    "Z = svc_rbf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "plt.scatter(X_nl[:, 0], X_nl[:, 1], c=y_nl, cmap='bwr', edgecolors='k')\n",
    "plt.title(\"Sklearn SVC (RBF) Baseline\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Takeaways\n",
    "*   **Hinge Loss vs Log Loss:** Hinge loss creates a \"margin\" by not penalizing points that are \"correct enough\" (score > 1), unlike Log Loss which always wants higher confidence.\n",
    "*   **Kernel Trick:** Allows us to separate the \"Moons\" dataset which is impossible for a linear classifier. We effectively lift the 2D data into infinite dimensions (RBF) where it is linearly separable.\n",
    "*   **Complexity:** The Kernel Perceptron predicts in $O(N \\cdot d)$ per query point, making it slower than linear models $O(d)$ for large datasets. This is why standard SVMs use support vectors (sparse $\\alpha$) to speed this up.\n",
    "*   **Linear vs RBF:** Use Linear for high-dimensional text data (efficient). Use RBF for complex low-dimensional boundaries (like the Moons example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "*   Explore **Unsupervised Learning**.\n",
    "*   [Go to K-Means Clustering](../unsupervised-learning/k-means-clustering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
