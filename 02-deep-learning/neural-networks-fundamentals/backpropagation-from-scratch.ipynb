{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation from Scratch (NumPy)\n",
    "**Objective:** Implement a Multilayer Perceptron (MLP) and Backpropagation from scratch to understand the chain rule and gradient descent updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup (Minimal)\n",
    "We will build a simple **MLP** for binary classification:\n",
    "1.  **Input Layer** ($X$)\n",
    "2.  **Hidden Layer:** Linear transform ($W_1, b_1$) -> ReLU activation\n",
    "3.  **Output Layer:** Linear transform ($W_2, b_2$) -> Sigmoid activation -> Prediction ($\\hat{y}$)\n",
    "\n",
    "**Backpropagation:** The method to compute gradients of the loss function with respect to weights using the **Chain Rule**.\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Binary Data (2 Gaussian Clusters)\n",
    "n_samples = 400\n",
    "\n",
    "# Cluster A (Class 0)\n",
    "X_0 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])\n",
    "y_0 = np.zeros((n_samples // 2, 1))\n",
    "\n",
    "# Cluster B (Class 1)\n",
    "X_1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])\n",
    "y_1 = np.ones((n_samples // 2, 1))\n",
    "\n",
    "# Combine and Shuffle\n",
    "X = np.vstack([X_0, X_1])\n",
    "y = np.vstack([y_0, y_1])\n",
    "\n",
    "# Shuffle\n",
    "indices = np.arange(n_samples)\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Train/Test Split (80/20)\n",
    "split_idx = int(0.8 * n_samples)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train.ravel(), cmap='bwr', alpha=0.6, edgecolors='k', label='Train')\n",
    "plt.title(\"Synthetic Binary Classification Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation (NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Clip to avoid overflow/underflow for extreme values\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_deriv(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def init_params(d_in, d_hidden, d_out):\n",
    "    \"\"\"Initialize weights using He Initialization (good for ReLU).\"\"\"\n",
    "    # He Init: scale by sqrt(2/fan_in)\n",
    "    W1 = np.random.randn(d_in, d_hidden) * np.sqrt(2 / d_in)\n",
    "    b1 = np.zeros((1, d_hidden))\n",
    "    \n",
    "    # Xavier Init for Sigmoid output (optional, but He is okay too)\n",
    "    W2 = np.random.randn(d_hidden, d_out) * np.sqrt(1 / d_hidden)\n",
    "    b2 = np.zeros((1, d_out))\n",
    "    \n",
    "    params = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return params\n",
    "\n",
    "def forward(X, params):\n",
    "    Z1 = X.dot(params[\"W1\"]) + params[\"b1\"]\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    Z2 = A1.dot(params[\"W2\"]) + params[\"b2\"]\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return cache, A2\n",
    "\n",
    "def compute_loss(y, y_prob):\n",
    "    \"\"\"Binary Cross Entropy Loss.\"\"\"\n",
    "    m = y.shape[0]\n",
    "    epsilon = 1e-15 # Prevent log(0)\n",
    "    loss = -1/m * np.sum(y * np.log(y_prob + epsilon) + (1 - y) * np.log(1 - y_prob + epsilon))\n",
    "    return loss\n",
    "\n",
    "def backward(X, y, y_prob, cache, params):\n",
    "    \"\"\"Computes gradients.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    W2 = params[\"W2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    Z1 = cache[\"Z1\"]\n",
    "    \n",
    "    # Output layer error (dZ2 = A2 - Y for Sigmoid + CrossEntropy)\n",
    "    dZ2 = y_prob - y\n",
    "    \n",
    "    # Gradients for Layer 2\n",
    "    dW2 = (1/m) * A1.T.dot(dZ2)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    # Hidden layer error\n",
    "    dA1 = dZ2.dot(W2.T)\n",
    "    dZ1 = dA1 * relu_deriv(Z1)\n",
    "    \n",
    "    # Gradients for Layer 1\n",
    "    dW1 = (1/m) * X.T.dot(dZ1)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "def update(params, grads, lr=0.1):\n",
    "    \"\"\"Update parameters using Gradient Descent.\"\"\"\n",
    "    params[\"W1\"] -= lr * grads[\"dW1\"]\n",
    "    params[\"b1\"] -= lr * grads[\"db1\"]\n",
    "    params[\"W2\"] -= lr * grads[\"dW2\"]\n",
    "    params[\"b2\"] -= lr * grads[\"db2\"]\n",
    "    return params\n",
    "\n",
    "def predict(X, params, threshold=0.5):\n",
    "    _, y_prob = forward(X, params)\n",
    "    return (y_prob >= threshold).astype(int)\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    return np.mean(y == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking (Validation)\n",
    "Verifying our analytical gradients against numerical gradients computed via finite differences.\n",
    "$$ \\frac{dJ}{d\\theta} \\approx \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(X, y):\n",
    "    \"\"\"Checks gradients for W1 and W2.\"\"\"\n",
    "    # Init small random network\n",
    "    params = init_params(d_in=2, d_hidden=5, d_out=1)\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    # Get Analytical Gradients\n",
    "    cache, y_prob = forward(X, params)\n",
    "    grads = backward(X, y, y_prob, cache, params)\n",
    "    \n",
    "    # Check a few random elements in W1\n",
    "    print(\"Checking Gradient for W1...\")\n",
    "    for _ in range(3):\n",
    "        row, col = np.random.randint(0, params[\"W1\"].shape[0]), np.random.randint(0, params[\"W1\"].shape[1])\n",
    "        \n",
    "        # Store original value\n",
    "        original_val = params[\"W1\"][row, col]\n",
    "        \n",
    "        # J(theta + eps)\n",
    "        params[\"W1\"][row, col] = original_val + epsilon\n",
    "        _, prob_plus = forward(X, params)\n",
    "        loss_plus = compute_loss(y, prob_plus)\n",
    "        \n",
    "        # J(theta - eps)\n",
    "        params[\"W1\"][row, col] = original_val - epsilon\n",
    "        _, prob_minus = forward(X, params)\n",
    "        loss_minus = compute_loss(y, prob_minus)\n",
    "        \n",
    "        # Numerical Grad\n",
    "        grad_numerical = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        \n",
    "        # Analytical Grad\n",
    "        grad_analytical = grads[\"dW1\"][row, col]\n",
    "        \n",
    "        # Relative Error\n",
    "        rel_error = abs(grad_analytical - grad_numerical) / (abs(grad_analytical) + abs(grad_numerical) + 1e-8)\n",
    "        \n",
    "        print(f\"  W1[{row},{col}]: Analytic={grad_analytical:.6f}, Numerical={grad_numerical:.6f}, RelError={rel_error:.2e}\")\n",
    "        \n",
    "        # Restore\n",
    "        params[\"W1\"][row, col] = original_val\n",
    "        \n",
    "    print(\"Gradient check passed if Relative Error is small (< 1e-5).\")\n",
    "\n",
    "# Run check on a small subset of data\n",
    "check_gradients(X_train[:5], y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "d_hidden = 10\n",
    "\n",
    "# Initialize\n",
    "params = init_params(d_in=2, d_hidden=d_hidden, d_out=1)\n",
    "loss_history = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    cache, y_prob = forward(X_train, params)\n",
    "    \n",
    "    # Loss\n",
    "    loss = compute_loss(y_train, y_prob)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Backward\n",
    "    grads = backward(X_train, y_train, y_prob, cache, params)\n",
    "    \n",
    "    # Update\n",
    "    params = update(params, grads, lr=learning_rate)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        y_pred = predict(X_train, params)\n",
    "        acc = accuracy(y_train, y_pred)\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}, Acc = {acc:.2f}\")\n",
    "\n",
    "# Final Evaluation\n",
    "y_pred_test = predict(X_test, params)\n",
    "test_acc = accuracy(y_test, y_pred_test)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary Cross Entropy Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, params):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                         np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    # Predict on whole grid\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    _, probs = forward(grid_points, params)\n",
    "    Z = probs.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='bwr', edgecolors='k')\n",
    "    plt.title(\"MLP Decision Boundary after Training\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_test, y_test, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Takeaways\n",
    "*   **Gradient Checking:** The low relative error (order of 1e-9 or less) confirms our chain rule implementation is mathematically correct. This is the gold standard for debugging neural nets.\n",
    "*   **Initialization:** We used 'He Initialization', which is standard for ReLU networks to keep the variance of activations similar across layers, preventing vanishing/exploding gradients initially.\n",
    "*   **Non-Linearity:** The decision boundary is curved (unlike Logistic Regression), showing the MLP successfully combined the ReLU neurons to separate the data.\n",
    "*   **Convergence:** The loss decreased smoothly. If the learning rate were too high, we might see oscillations; too low, and it would stall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "*   Now that we have the mechanics, we need better **Optimization Algorithms** than plain SGD.\n",
    "*   [Go to Optimization Algorithms (Momentum, Adam)](./optimization-algorithms.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
